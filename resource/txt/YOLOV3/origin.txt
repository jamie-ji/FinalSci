https://pjreddie.com/yolo/.
1. Introduction
Sometimes you just kinda phone it in for a year, you
know? I didn’t do a whole lot of research this year. Spent
a lot of time on Twitter. Played around with GANs a little.
I had a little momentum left over from last year [12] [1]; I
managed to make some improvements to YOLO. But, hon-
estly, nothing like super interesting, just a bunch of small
changes that make it better.
I also helped out with other
people’s research a little.
Actually, that’s what brings us here today. We have
a camera-ready deadline [4] and we need to cite some of
the random updates I made to YOLO but we don’t have a
source. So get ready for a TECH REPORT!
The great thing about tech reports is that they don’t need
intros, y’all know why we’re here. So the end of this intro-
duction will signpost for the rest of the paper. First we’ll tell
you what the deal is with YOLOv3. Then we’ll tell you how
we do. We’ll also tell you about some things we tried that
didn’t work. Finally we’ll contemplate what this all means.
2. The Deal
So here’s the deal with YOLOv3: We mostly took good
ideas from other people. We also trained a new classiﬁer
network that’s better than the other ones. We’ll just take
you through the whole system from scratch so you can un-
derstand it all.
2.1. Bounding Box Prediction
Following YOLO9000 our system predicts bounding
boxes using dimension clusters as anchor boxes [15]. The
network predicts 4 coordinates for each bounding box, tx ,
ty , tw , th . If the cell is offset from the top left corner of the
image by (cx , cy ) and the bounding box prior has width and
height pw , ph , then the predictions correspond to:
bx = σ(tx ) + cx
by = σ(ty ) + cy
bw = pw etw
bh = ph eth

Next we take the feature map from 2 layers previous and
upsample it by 2×. We also take a feature map from earlier
in the network and merge it with our upsampled features
using concatenation. This method allows us to get more
meaningful semantic information from the upsampled fea-
tures and ﬁner-grained information from the earlier feature
map. We then add a few more convolutional layers to pro-
cess this combined feature map, and eventually predict a
similar tensor, although now twice the size.
We perform the same design one more time to predict
boxes for the ﬁnal scale. Thus our predictions for the 3rd
scale beneﬁt from all the prior computation as well as ﬁne-
grained features from early on in the network.
We still use k-means clustering to determine our bound-
ing box priors. We just sort of chose 9 clusters and 3
scales arbitrarily and then divide up the clusters evenly
across scales. On the COCO dataset the 9 clusters were:
(10 × 13), (16 × 30), (33 × 23), (30 × 61), (62 × 45), (59 ×
119), (116 × 90), (156 × 198), (373 × 326).
2.4. Feature Extractor
prediction. We predict the width and height of the box as offsets
from cluster centroids. We predict the center coordinates of the
box relative to the location of ﬁlter application using a sigmoid
function. This ﬁgure blatantly self-plagiarized from [15].
is not the best but does overlap a ground truth object by
more than some threshold we ignore the prediction, follow-
ing [17]. We use the threshold of .5. Unlike [17] our system
only assigns one bounding box prior for each ground truth
object. If a bounding box prior is not assigned to a ground
truth object it incurs no loss for coordinate or class predic-
tions, only objectness.
We use a new network for performing feature extraction.
Our new network is a hybrid approach between the network
used in YOLOv2, Darknet-19, and that newfangled residual
network stuff. Our network uses successive 3 × 3 and 1 × 1
convolutional layers but now has some shortcut connections
as well and is signiﬁcantly larger. It has 53 convolutional
layers so we call it.... wait for it..... Darknet-53!
2.2. Class Prediction
Each box predicts the classes the bounding box may con-
tain using multilabel classiﬁcation. We do not use a softmax
as we have found it is unnecessary for good performance,
instead we simply use independent logistic classiﬁers. Dur-
ing training we use binary cross-entropy loss for the class
predictions.
This formulation helps when we move to more complex
domains like the Open Images Dataset [7]. In this dataset
there are many overlapping labels (i.e. Woman and Person).
Using a softmax imposes the assumption that each box has
exactly one class which is often not the case. A multilabel
approach better models the data.
2.3. Predictions Across Scales
YOLOv3 predicts boxes at 3 different scales. Our sys-
tem extracts features from those scales using a similar con-
cept to feature pyramid networks [8]. From our base fea-
ture extractor we add several convolutional layers. The last
of these predicts a 3-d tensor encoding bounding box, ob-
jectness, and class predictions.
In our experiments with
COCO [10] we predict 3 boxes at each scale so the tensor is
N × N × [3 ∗ (4 + 1 + 80)] for the 4 bounding box offsets,
1 objectness prediction, and 80 class predictions.
This new network is much more powerful than Darknet-
19 but still more efﬁcient than ResNet-101 or ResNet-152.
Here are some ImageNet results:
Backbone
Darknet-19 [15]
ResNet-101[5]
ResNet-152 [5]
Darknet-53
Top-1
74.1
77.1
77.6
77.2
Top-5 Bn Ops BFLOP/s
91.8
7.29
1246
93.7
19.7
1039
29.4
1090
18.7
93.8
93.8
1457
FPS
171
53
37
78
Each network is trained with identical settings and tested
at 256 × 256, single crop accuracy. Run times are measured
on a Titan X at 256 × 256. Thus Darknet-53 performs on
par with state-of-the-art classiﬁers but with fewer ﬂoating
point operations and more speed. Darknet-53 is better than
ResNet-101 and 1.5× faster. Darknet-53 has similar perfor-
mance to ResNet-152 and is 2× faster.
Darknet-53 also achieves the highest measured ﬂoating
point operations per second. This means the network struc-
ture better utilizes the GPU, making it more efﬁcient to eval-
uate and thus faster. That’s mostly because ResNets have
just way too many layers and aren’t very efﬁcient.
2.5. Training
We still train on full images with no hard negative mining
or any of that stuff. We use multi-scale training, lots of data
augmentation, batch normalization, all the standard stuff.
We use the Darknet neural network framework for training
and testing [14].
3. How We Do
YOLOv3 is pretty good! See table 3. In terms of COCOs
weird average mean AP metric it is on par with the SSD
variants but is 3× faster. It is still quite a bit behind other
models like RetinaNet in this metric though.
However, when we look at the “old” detection metric of
mAP at IOU= .5 (or AP50 in the chart) YOLOv3 is very
strong.
It is almost on par with RetinaNet and far above
the SSD variants. This indicates that YOLOv3 is a very
strong detector that excels at producing decent boxes for ob-
jects. However, performance drops signiﬁcantly as the IOU
threshold increases indicating YOLOv3 struggles to get the
boxes perfectly aligned with the object.
In the past YOLO struggled with small objects. How-
ever, now we see a reversal in that trend. With the new
multi-scale predictions we see YOLOv3 has relatively high
APS performance. However, it has comparatively worse
performance on medium and larger size objects. More in-
vestigation is needed to get to the bottom of this.
When we plot accuracy vs speed on the AP50 metric (see
ﬁgure 5) we see YOLOv3 has signiﬁcant beneﬁts over other
detection systems. Namely, it’s faster and better.
4. Things We Tried That Didn’t Work
We tried lots of stuff while we were working on
YOLOv3. A lot of it didn’t work. Here’s the stuff we can
remember.
Anchor box x, y offset predictions. We tried using the
normal anchor box prediction mechanism where you pre-
dict the x, y offset as a multiple of the box width or height
using a linear activation. We found this formulation de-
creased model stability and didn’t work very well.
Linear x, y predictions instead of logistic. We tried
using a linear activation to directly predict the x, y offset
instead of the logistic activation. This led to a couple point
drop in mAP.
Focal loss. We tried using focal loss.
It dropped our
mAP about 2 points. YOLOv3 may already be robust to
the problem focal loss is trying to solve because it has sep-
arate objectness predictions and conditional class predic-
tions. Thus for most examples there is no loss from the
class predictions? Or something? We aren’t totally sure.
Two-stage methods
Faster R-CNN+++ [5]
Faster R-CNN w FPN [8]
Faster R-CNN by G-RMI [6]
Faster R-CNN w TDM [20]
One-stage methods
YOLOv2 [15]
SSD513 [11, 3]
DSSD513 [3]
RetinaNet [9]
RetinaNet [9]
YOLOv3 608 × 608
backbone

AP50
AP75
APS
APM
APL
ResNet-101-C4
34.9
ResNet-101-FPN
36.2
Inception-ResNet-v2 [21]
34.7
Inception-ResNet-v2-TDM 36.8
DarkNet-19 [15]
ResNet-101-SSD
ResNet-101-DSSD
ResNet-101-FPN
ResNeXt-101-FPN
Darknet-53
21.6
31.2
33.2
39.1
40.8
33.0
55.7
59.1
55.5
57.7
44.0
50.4
53.3
59.1
61.1
57.9
37.4
39.0
36.7
39.2
19.2
33.3
35.2
42.3
44.1
34.4
15.6
18.2
13.5
16.2
5.0
10.2
13.0
21.8
24.1
18.3
38.7
39.0
38.1
39.8
22.4
34.5
35.4
42.7
44.2
35.4
50.9
48.2
52.0
52.1
35.5
49.8
51.1
50.2
51.2
41.9
Dual IOU thresholds and truth assignment. Faster R-
CNN uses two IOU thresholds during training. If a predic-
tion overlaps the ground truth by .7 it is as a positive exam-
ple, by [.3 − .7] it is ignored, less than .3 for all ground truth
objects it is a negative example. We tried a similar strategy
but couldn’t get good results.
We quite like our current formulation, it seems to be at
a local optima at least.
It is possible that some of these
techniques could eventually produce good results, perhaps
they just need some tuning to stabilize the training.
5. What This All Means
YOLOv3 is a good detector. It’s fast, it’s accurate. It’s
not as great on the COCO average AP between .5 and .95
IOU metric. But it’s very good on the old detection metric
of .5 IOU.
Why did we switch metrics anyway? The original
COCO paper just has this cryptic sentence: “A full discus-
sion of evaluation metrics will be added once the evaluation
server is complete”. Russakovsky et al report that that hu-
mans have a hard time distinguishing an IOU of .3 from .5!
“Training humans to visually inspect a bounding box with
IOU of 0.3 and distinguish it from one with IOU 0.5 is sur-
1 The author is funded by the Ofﬁce of Naval Research and Google.
[18] O. Russakovsky, L.-J. Li, and L. Fei-Fei. Best of both
worlds: human-machine collaboration for object annotation.
In Proceedings of the IEEE Conference on Computer Vision
and Pattern Recognition, pages 2121–2131, 2015. 4
[19] M. Scott. Smart camera gimbal bot scanlime:027, Dec 2017.
4
[20] A. Shrivastava, R. Sukthankar, J. Malik, and A. Gupta. Be-
yond skip connections: Top-down modulation for object de-
tection. arXiv preprint arXiv:1612.06851, 2016. 3
[21] C. Szegedy, S. Ioffe, V. Vanhoucke, and A. A. Alemi.
Inception-v4, inception-resnet and the impact of residual
connections on learning. 2017. 3
References
[1] Analogy. Wikipedia, Mar 2018. 1
[2] M. Everingham, L. Van Gool, C. K. Williams, J. Winn, and
A. Zisserman. The pascal visual object classes (voc) chal-
lenge. International journal of computer vision, 88(2):303–
338, 2010. 6
[3] C.-Y. Fu, W. Liu, A. Ranga, A. Tyagi, and A. C. Berg.
Dssd: Deconvolutional single shot detector. arXiv preprint
arXiv:1701.06659, 2017. 3
[4] D. Gordon, A. Kembhavi, M. Rastegari, J. Redmon, D. Fox,
and A. Farhadi. Iqa: Visual question answering in interactive
environments. arXiv preprint arXiv:1712.03316, 2017. 1
[5] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learn-
ing for image recognition. In Proceedings of the IEEE con-
ference on computer vision and pattern recognition, pages
770–778, 2016. 3
[6] J. Huang, V. Rathod, C. Sun, M. Zhu, A. Korattikara,
A. Fathi, I. Fischer, Z. Wojna, Y. Song, S. Guadarrama, et al.
Speed/accuracy trade-offs for modern convolutional object
detectors. 3
[7] I. Krasin, T. Duerig, N. Alldrin, V. Ferrari, S. Abu-El-Haija,
A. Kuznetsova, H. Rom, J. Uijlings, S. Popov, A. Veit,
S. Belongie, V. Gomes, A. Gupta, C. Sun, G. Chechik,
D. Cai, Z. Feng, D. Narayanan, and K. Murphy. Open-
images: A public dataset for large-scale multi-label and
multi-class image classiﬁcation. Dataset available from
https://github.com/openimages, 2017. 2
[8] T.-Y. Lin, P. Dollar, R. Girshick, K. He, B. Hariharan, and
S. Belongie. Feature pyramid networks for object detection.
In Proceedings of the IEEE Conference on Computer Vision
and Pattern Recognition, pages 2117–2125, 2017. 2, 3
[9] T.-Y. Lin, P. Goyal, R. Girshick, K. He, and P. Doll ´ar.
Focal
loss for dense object detection.
arXiv preprint
arXiv:1708.02002, 2017. 1, 3, 4
[10] T.-Y. Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ra-
manan, P. Doll ´ar, and C. L. Zitnick. Microsoft coco: Com-
mon objects in context. In European conference on computer
vision, pages 740–755. Springer, 2014. 2
[11] W. Liu, D. Anguelov, D. Erhan, C. Szegedy, S. Reed, C.-
Y. Fu, and A. C. Berg. Ssd: Single shot multibox detector.
In European conference on computer vision, pages 21–37.
Springer, 2016. 3
[12] I. Newton. Philosophiae naturalis principia mathematica.
William Dawson & Sons Ltd., London, 1687. 1
[13] J. Parham, J. Crall, C. Stewart, T. Berger-Wolf, and
D. Rubenstein. Animal population censusing at scale with
citizen science and photographic identiﬁcation. 2017. 4
[14] J. Redmon. Darknet: Open source neural networks in c.
http://pjreddie.com/darknet/, 2013–2016. 3
[15] J. Redmon and A. Farhadi. Yolo9000: Better, faster, stronger.
In Computer Vision and Pattern Recognition (CVPR), 2017
IEEE Conference on, pages 6517–6525. IEEE, 2017. 1, 2, 3
[16] J. Redmon and A. Farhadi. Yolov3: An incremental improve-
ment. arXiv, 2018. 4
[17] S. Ren, K. He, R. Girshick, and J. Sun. Faster r-cnn: To-
wards real-time object detection with region proposal net-
works. arXiv preprint arXiv:1506.01497, 2015. 2
Rebuttal
precise bounding boxes are more important than better classiﬁ-
cation? A miss-classiﬁed example is much more obvious than a
bounding box that is slightly shifted.
mAP is already screwed up because all that matters is per-class
rank ordering. For example, if your test set only has these two
images then according to mAP two detectors that produce these
results are JUST AS GOOD:
Now this is OBVIOUSLY an over-exaggeration of the prob-
lems with mAP but I guess my newly retconned point is that there
are such obvious discrepancies between what people in the “real
world” would care about and our current metrics that I think if
we’re going to come up with new metrics we should focus on
these discrepancies. Also, like, it’s already mean average preci-
sion, what do we even call the COCO metric, average mean aver-
age precision?
Here’s a proposal, what people actually care about is given an
image and a detector, how well will the detector ﬁnd and classify
objects in the image. What about getting rid of the per-class AP
and just doing a global average precision? Or doing an AP calcu-
lation per-image and averaging over that?
Boxes are stupid anyway though, I’m probably a true believer
in masks except I can’t get YOLO to learn them.
