				标题为:bart: denoising sequence-to-sequence pre-training for natural
language generation, translation, and comprehension

一作为:Facebook AI
abstract：We present BART, a denoising autoencoderfor pretraining sequence-to-sequence models.BART is trained by (1) corrupting text with anarbitrary noising function, and (2) learning amodel to reconstruct the original text. It usesa standard Tranformer-based neural machinetranslation architecture which, despite its sim-plicity, can be seen as generalizing BERT (dueto the bidirectional encoder), GPT (with theleft-to-right decoder), and many other more re-cent pretraining schemes. We evaluate a num-ber of noising approaches, ﬁnding the best per-formance by both randomly shufﬂing the or-der of the original sentences and using a novelin-ﬁlling scheme, where spans of text are re-placed with a single mask token. BART isparticularly effective when ﬁne tuned for textgeneration but also works well for compre-hension tasks. It matches the performance ofRoBERTa with comparable training resourceson GLUE and SQuAD, achieves new state-of-the-art results on a range of abstractive di-alogue, question answering, and summariza-tion tasks, with gains of up to 6 ROUGE.BART also provides a 1.1 BLEU increase overa back-translation system for machine transla-tion, with only target language pretraining. Wealso report ablation experiments that replicateother pretraining schemes within the BARTframework, to better measure which factorsmost inﬂuence end-task performance.

摘要：我们提出了一种用于序列对序列模型预训练的去噪自编码器BART。BART通过(1)带有任意噪声函数的破坏文本和(2)学习模型来重建原始文本。它使用了一种标准的基于transformer的神经机器翻译架构，尽管它很简单，但可以被视为通用BERT(由于双向编码器)、GPT(具有从左到右的解码器)和许多其他最新的预训练方案。我们评估了多种降噪方法，通过随机打乱原始句子的顺序和使用novelin填充方案(使用单个掩码标记替换文本范围)找到最佳性能。当对文本生成进行微调时，BART尤其有效，但也适用于理解任务。它将froberta的性能与GLUE和SQuAD的可比培训资源相匹配，在一系列抽象对话、问题回答和总结任务方面取得了最新的成果，收益高达6 ROUGE。BART还比机器翻译的反翻译系统增加了1.1 BLEU，只进行了目标语言的预训练。我们还报告了消融实验，在巴特框架内复制了其他预训练方案，以更好地衡量哪些因素对最终任务性能的影响最大。
